{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation\n",
    "\n",
    "(1)hidden_size: output dimension \n",
    "\n",
    "-> result of hidden_state: h_t.shape = (hidden_size, 1)\n",
    "\n",
    "(2)vstack: stack in row direction\n",
    "\n",
    " -> rsult of vstack: concat_vector.shape = (input_size + hidden_size , 1)\n",
    "\n",
    "* but it can only same size col.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class activation_function:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "class LSTM(activation_function):\n",
    "    def __init__(self, hidden_size, concat_size):\n",
    "        # z  = alpha(w @ [h, x] + b) \n",
    "        # alpha = activation function\n",
    "        self.hidden_size = hidden_size\n",
    "        self.concat_size = concat_size\n",
    "        self.w_f = np.random.randn(hidden_size, concat_size)\n",
    "        self.b_f = np.zeros((hidden_size, 1))\n",
    "        self.w_o = np.random.randn(hidden_size, concat_size)\n",
    "        self.b_o = np.zeros((hidden_size, 1))\n",
    "        self.w_c = np.random.randn(hidden_size, concat_size)\n",
    "        self.b_c = np.zeros((hidden_size, 1))\n",
    "        self.w_i = np.random.randn(hidden_size, concat_size)\n",
    "        self.b_i = np.zeros((hidden_size, 1))\n",
    "        self.cache = None\n",
    "    \n",
    "    def forget_gate(self, concat_vector):\n",
    "        value = (self.w_f @ concat_vector) + self.b_f\n",
    "        f_t = self.sigmoid(value)\n",
    "        return f_t\n",
    "    \n",
    "    def input_gate(self, concat_vector):\n",
    "        value = (self.w_i @ concat_vector) + self.b_i\n",
    "        i_t = self.sigmoid(value)\n",
    "        return i_t\n",
    "    \n",
    "    def candidate_memory(self, concat_vector):\n",
    "        value = (self.w_c @ concat_vector) + self.b_c\n",
    "        c_t_hat = np.tanh(value)\n",
    "        return c_t_hat\n",
    "    \n",
    "    def output_gate(self ,concat_vector):\n",
    "        value = (self.w_o @ concat_vector) + self.b_o\n",
    "        o_t = self.sigmoid(value)\n",
    "        return o_t\n",
    "\n",
    "    def cell_state_update(self, c_prev ,concat_vector):\n",
    "        c_t_hat = self.candidate_memory(concat_vector)\n",
    "        f_t = self.forget_gate(concat_vector)\n",
    "        i_t = self.input_gate(concat_vector)\n",
    "        c_t = (f_t * c_prev) + (i_t * c_t_hat)\n",
    "        return c_t\n",
    "    \n",
    "    #print summarized information\n",
    "    def hidden_state(self, c_t, concat_vector):\n",
    "        h_t = self.output_gate(concat_vector) * np.tanh(c_t)\n",
    "        return h_t\n",
    "    \n",
    "    def forward(self, h_prev, x_t , c_prev):\n",
    "        concat_vector = np.vstack((h_prev, x_t))\n",
    "        c_t = self.cell_state_update(c_prev, concat_vector)\n",
    "        h_t = self.hidden_state(c_t, concat_vector)\n",
    "        self.cache = {\n",
    "            'h_prev': h_prev,\n",
    "            'x_t': x_t,\n",
    "            'c_prev': c_prev,\n",
    "            'f_t': self.forget_gate(concat_vector),\n",
    "            'i_t': self.input_gate(concat_vector),\n",
    "            'c_t_hat': self.cell_state_update(c_prev, concat_vector),\n",
    "            'o_t': self.output_gate(concat_vector),\n",
    "            'c_t': c_t,\n",
    "            'h_t': h_t,\n",
    "            'concat': concat_vector\n",
    "        }\n",
    "        return h_t, c_t\n",
    "    \n",
    "    def backward(self, dh, dc_next=None):\n",
    "        # 가져오기\n",
    "        f_t = self.cache['f_t']\n",
    "        i_t = self.cache['i_t']\n",
    "        c_t_hat = self.cache['c_t_hat']\n",
    "        o_t = self.cache['o_t']\n",
    "        c_t = self.cache['c_t']\n",
    "        c_prev = self.cache['c_prev']\n",
    "        h_prev = self.cache['h_prev']\n",
    "        x_t = self.cache['x_t']\n",
    "        concat = self.cache['concat']\n",
    "\n",
    "        # c_t에 대한 미분 (from dh)\n",
    "        dtanh = o_t * (1 - np.tanh(c_t)**2)\n",
    "        dc = dh * dtanh  # ∂L/∂c_t\n",
    "\n",
    "        if dc_next is not None:\n",
    "            dc += dc_next  # c_t는 이전 time step에도 영향을 주기 때문에 합산\n",
    "\n",
    "        # 각 게이트에 대한 미분\n",
    "        do = dh * np.tanh(c_t)\n",
    "        do_raw = do * o_t * (1 - o_t)\n",
    "\n",
    "        df = dc * c_prev\n",
    "        df_raw = df * f_t * (1 - f_t)\n",
    "\n",
    "        di = dc * c_t_hat\n",
    "        di_raw = di * i_t * (1 - i_t)\n",
    "\n",
    "        dc_hat = dc * i_t\n",
    "        dc_hat_raw = dc_hat * (1 - c_t_hat**2)\n",
    "\n",
    "        # weight gradient 계산\n",
    "        self.dw_f = df_raw @ concat.T\n",
    "        self.db_f = df_raw\n",
    "\n",
    "        self.dw_i = di_raw @ concat.T\n",
    "        self.db_i = di_raw\n",
    "\n",
    "        self.dw_c = dc_hat_raw @ concat.T\n",
    "        self.db_c = dc_hat_raw\n",
    "\n",
    "        self.dw_o = do_raw @ concat.T\n",
    "        self.db_o = do_raw\n",
    "\n",
    "        # 다음 타임스텝으로 넘겨줄 gradient\n",
    "        dconcat = (self.w_f.T @ df_raw +\n",
    "                self.w_i.T @ di_raw +\n",
    "                self.w_c.T @ dc_hat_raw +\n",
    "                self.w_o.T @ do_raw)  # shape: (hidden + input, 1)\n",
    "\n",
    "        dh_prev = dconcat[:self.hidden_size, :]\n",
    "        dx = dconcat[self.hidden_size:, :]\n",
    "        dc_prev = dc * f_t\n",
    "\n",
    "        return dh_prev, dc_prev\n",
    "\n",
    "    def update(self, lr = 0.01):\n",
    "        self.w_f -= lr * self.dw_f\n",
    "        self.b_f -= lr * self.db_f\n",
    "\n",
    "        self.w_i -= lr * self.dw_i\n",
    "        self.b_i -= lr * self.db_i\n",
    "\n",
    "        self.w_c -= lr * self.dw_c\n",
    "        self.b_c -= lr * self.db_c\n",
    "\n",
    "        self.w_o -= lr * self.dw_o\n",
    "        self.b_o -= lr * self.db_o\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# example_data_set\n",
    "\n",
    "(1)lstm only get 3_dimension data_set\n",
    "    \n",
    "    -> we need to data process\n",
    "\n",
    "(2) because data is just an array of numbers,\n",
    "\n",
    "* we need to create a rule to input into lstm.\n",
    "\n",
    "    def create_sequences divides an array of numbers into time_steps,\n",
    "\n",
    "    and creates multiple samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 6) (4,)\n",
      "(4, 6, 1)\n"
     ]
    }
   ],
   "source": [
    "train_set = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "test_set = [1, 2, 3, 4, 5, 6]\n",
    "train_set = np.array(train_set)\n",
    "test_set = np.array(test_set)\n",
    "\n",
    "def create_sequences(data, time_steps =6):\n",
    "    x, y = [], []\n",
    "    for i in range(len(data) - time_steps): #10 - 3 = 7\n",
    "        seq_x = data[i:i + time_steps]\n",
    "        #0~6, 1~7, 2~8, 3~9, 4~10 (sample)\n",
    "        seq_y = data[i + time_steps]\n",
    "        #7, 8, 9, 10 (correct label)\n",
    "        x.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(x), np.array(y)\n",
    "\n",
    "#==================make matrix==================\n",
    "train_data_x , train_data_y = create_sequences(train_set)\n",
    "#==================check data shape=============\n",
    "print(train_data_x.shape, train_data_y.shape)\n",
    "\n",
    "#==================reshape 3_dimension==========\n",
    "train_data_x = train_data_x.reshape((train_data_x.shape[0], train_data_x.shape[1], 1))\n",
    "print(train_data_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ex_1 basic lstm\n",
    "\n",
    "- this result is not accurate. \n",
    "\n",
    "    so we need backward definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_size = 1\n",
    "# hidden_size = 4\n",
    "# lstm = LSTM(input_size, concat_size = input_size + hidden_size)\n",
    "\n",
    "# h = np.zeros((hidden_size, 1))\n",
    "# c = np.zeros((hidden_size, 1))\n",
    "\n",
    "# last_seq = test_set[-3: ]\n",
    "# for i in range(100):\n",
    "#     for i in last_seq:\n",
    "#         x_t = np.array([i])\n",
    "#         h, c = lstm.forward(h, x_t, c)\n",
    "\n",
    "# next_value = h.mean()\n",
    "# print(next_value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense & backward\n",
    "\n",
    "(1) dense class is reshape lstm result with scalar\n",
    "\n",
    "(2) calculator class is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense:\n",
    "    def __init__(self, input_size):\n",
    "        self.w = np.random.randn(1, input_size) * 0.01\n",
    "        self.b = np.zeros((1,1))\n",
    "        self.lr = 0.01\n",
    "    \n",
    "    def forward(self, h):\n",
    "        self.h = h\n",
    "        y_pred = self.w @ h + self.b\n",
    "        return y_pred\n",
    "    \n",
    "    def backward(self, dy):\n",
    "        self.dw = dy @ self.h.T\n",
    "        self.db = dy\n",
    "        self.dh = self.w.T @ dy\n",
    "        return self.dh\n",
    "    \n",
    "    def update(self):\n",
    "        self.w -= self.lr * self.dw\n",
    "        self.b -= self.lr * self.db\n",
    "\n",
    "class calculator:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def mse_loss(self, y_pred, y_true):\n",
    "        return np.mean((y_pred - y_true) ** 2)\n",
    "    \n",
    "    def mse_grad(self, y_pred, y_true):\n",
    "        return 2 * (y_pred - y_true)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] Loss: 69.0937\n",
      "[100] Loss: 0.9174\n",
      "[200] Loss: 0.9844\n",
      "[300] Loss: 0.7992\n",
      "[400] Loss: 0.3992\n",
      "[500] Loss: 0.3049\n",
      "[600] Loss: 0.1400\n",
      "[700] Loss: 0.1324\n",
      "[800] Loss: 0.1130\n",
      "[900] Loss: 0.1114\n",
      "7.1891586492689585\n"
     ]
    }
   ],
   "source": [
    "input_size = 1\n",
    "hidden_size = 8\n",
    "lstm = LSTM(hidden_size, concat_size = hidden_size + input_size)\n",
    "dense = Dense(input_size = hidden_size)\n",
    "cal = calculator()\n",
    "epochs = 1000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for i in range(len(train_data_x)):\n",
    "        x_seq = train_data_x[i]\n",
    "        y_true = train_data_y[i]\n",
    "\n",
    "        h = np.zeros((hidden_size, 1))\n",
    "        c = np.zeros((hidden_size, 1))\n",
    "        \n",
    "        for t in range(x_seq.shape[0]):\n",
    "            x_t = x_seq[t].reshape(-1,1)\n",
    "            h, c = lstm.forward(h, x_t, c)\n",
    "\n",
    "        y_pred = dense.forward(h)\n",
    "        loss = cal.mse_loss(y_pred, y_true)\n",
    "        dy = cal.mse_grad(y_pred, y_true)\n",
    "\n",
    "        dh = dense.backward(dy)\n",
    "        dh, dc = lstm.backward(dh)\n",
    "\n",
    "        dense.update()\n",
    "        lstm.update()\n",
    "\n",
    "        total_loss += loss\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"[{epoch}] Loss: {total_loss/len(train_data_x):.4f}\")\n",
    "\n",
    "test_input = test_set\n",
    "h = np.zeros((hidden_size, 1))\n",
    "c = np.zeros((hidden_size, 1))\n",
    "for t in range(test_input.shape[0]):\n",
    "    x_t = test_input[t].reshape(1,1)\n",
    "    h, c = lstm.forward(h, x_t, c)\n",
    "\n",
    "y_pred = dense.forward(h)\n",
    "print(y_pred.item())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
